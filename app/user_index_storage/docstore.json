{"docstore/metadata": {"8b216cc1-d4ca-4411-95cc-d2ea4a2ed248": {"doc_hash": "fdd6ff52da4848226867d7bd74523c9ed4bbd8f969bbd83f38778f03552c417e"}, "dc66679b-e240-4071-bd33-7e9b92bf53eb": {"doc_hash": "57021d2fac6833892bd703bf725567745a4a6ac401a7f81c7bcf13945437bb80", "ref_doc_id": "8b216cc1-d4ca-4411-95cc-d2ea4a2ed248"}, "2c4e42d2-4860-47ae-ac62-41bfed88dbd8": {"doc_hash": "5f7201ece7fd7372681ef456329573a9d726cc222d3f1fd23daf7eac58615776", "ref_doc_id": "8b216cc1-d4ca-4411-95cc-d2ea4a2ed248"}}, "docstore/data": {"dc66679b-e240-4071-bd33-7e9b92bf53eb": {"__data__": {"id_": "dc66679b-e240-4071-bd33-7e9b92bf53eb", "embedding": null, "metadata": {"file_name": "FL for LLM ResearchPaper.docx", "file_path": "C:\\Users\\Archit\\Desktop\\help\\llama_index-app\\app\\user_uploads\\FL for LLM ResearchPaper.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 15442, "creation_date": "2024-11-20", "last_modified_date": "2024-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8b216cc1-d4ca-4411-95cc-d2ea4a2ed248", "node_type": "4", "metadata": {"file_name": "FL for LLM ResearchPaper.docx", "file_path": "C:\\Users\\Archit\\Desktop\\help\\llama_index-app\\app\\user_uploads\\FL for LLM ResearchPaper.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 15442, "creation_date": "2024-11-20", "last_modified_date": "2024-11-20"}, "hash": "fdd6ff52da4848226867d7bd74523c9ed4bbd8f969bbd83f38778f03552c417e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2c4e42d2-4860-47ae-ac62-41bfed88dbd8", "node_type": "1", "metadata": {}, "hash": "15cf24a11187063e26d1d0a3df6d09d81219fc4a388a5641f0fe2f91b861842e", "class_name": "RelatedNodeInfo"}}, "text": "Federated Learning for Large Language Models\n\n\n\nArchit Ojha\n\nSchool of CSE andTechnology Bennett University Greater Noida, India\n\n\n\n\nAryan Singh \n\nSchool of CSE and Technology\n\nBennett University Greater Noida, India\n\n\n\n\n\nVaibhav Ahlawat\n\nSchool of CSE and Technology Bennett University Greater Noida, India\n\n\n\n\n\nSoumay Choudhry\n\nSchool of CSE and Technology Bennett University Greater Noida, India\n\n\n\n\n\n\tIntroduction\n\n\t\n\nAs data collection in the ML field increases, storing and managing data is not the only problem we face today. Privacy and security of data are also taken into consideration. Privacy of one\u2019s personal information has always been a priority, and over the years, many works have been done to improve security and data privacy. The nature of most of the data is privacy-sensitive, to overcome this data breach problem, the Federated learning approach comes into play. Federated learning is an approach of Machine Learning in which data sets are trained on multiple edge devices locally without any sharing of data.\n\nFederated learning implementation has significantly impacted data privacy and security and  made the training of data sets easier. As previously ,centralized methods were used in which data was trained and a model was created on the one server which may expose the personal information and data. But with Federated learning models are trained on edge devices and with no sharing of data to another server no personal data is exposed.\n\n\n\nIt allows total control of the model on the edge user i.e. which information to share, remove or hide all actions are taken on the user's consent which make it a very reliable form of learning for  training the large models. Privacy-perspective wise private data doesn't leave the device and only meaningful data is used in the training which makes it very privacy friendly.However, Federated learning is vulnerable to poisoning attacks which aim at causing convergence to the wrong model. Federated learning can be classified into horizontal,vertical and transfer learning based on their distribution.horizontal-same features but different samples,vertical -same user but different features,transfer-not having enough data to train. Federated learning distributed deep learning by eliminating central approach techniques and training  data set on their devices.With the increase in IoT devices and edge devices the collection of data has become efficient and for large data Federated learning is best for model training over the traditional methods.\n\n\n\nAll the measures and privacy safeguards ensure that the Federated learning does not violate the GDPR or another data based law.\n\n\n\nThis technique of training models on decentralized data involves a central server and a group of clients. Clients are compute nodes that perform local training using their local data. The central server first sends a standard global model to a group of clients. Clients then train the global model with local data and provide local models back to the server. The server aggregates the local models into a new global model and then starts a new training round. This process may be repeated several times until the global model converges or a certain threshold is reached.\n\n\n\nFederated optimization has several key properties that differentiate it from a typical distributed optimization problem:\n\n\n\n \u2022 Non-IID:  The training data on a given client is typically based on the usage of the mobile device by a particular user, and hence any particular user\u2019s local dataset will not be representative of the population distribution.\n\n\n\n\u2022 Unbalanced Similarly: some users will make much heavier use of the service or app than others, leading to varying amounts of local training data.\n\n\n\n \u2022 Massively distributed: We expect the number of clients participating in an optimization to be much larger than the average number of examples per client.\n\n\n\n \u2022 Limited communication: Mobile devices are frequently offline or on slow or expensive connections. \n\n                                                                            Table 1: Literature Review", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4103, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2c4e42d2-4860-47ae-ac62-41bfed88dbd8": {"__data__": {"id_": "2c4e42d2-4860-47ae-ac62-41bfed88dbd8", "embedding": null, "metadata": {"file_name": "FL for LLM ResearchPaper.docx", "file_path": "C:\\Users\\Archit\\Desktop\\help\\llama_index-app\\app\\user_uploads\\FL for LLM ResearchPaper.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 15442, "creation_date": "2024-11-20", "last_modified_date": "2024-11-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8b216cc1-d4ca-4411-95cc-d2ea4a2ed248", "node_type": "4", "metadata": {"file_name": "FL for LLM ResearchPaper.docx", "file_path": "C:\\Users\\Archit\\Desktop\\help\\llama_index-app\\app\\user_uploads\\FL for LLM ResearchPaper.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 15442, "creation_date": "2024-11-20", "last_modified_date": "2024-11-20"}, "hash": "fdd6ff52da4848226867d7bd74523c9ed4bbd8f969bbd83f38778f03552c417e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dc66679b-e240-4071-bd33-7e9b92bf53eb", "node_type": "1", "metadata": {"file_name": "FL for LLM ResearchPaper.docx", "file_path": "C:\\Users\\Archit\\Desktop\\help\\llama_index-app\\app\\user_uploads\\FL for LLM ResearchPaper.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 15442, "creation_date": "2024-11-20", "last_modified_date": "2024-11-20"}, "hash": "57021d2fac6833892bd703bf725567745a4a6ac401a7f81c7bcf13945437bb80", "class_name": "RelatedNodeInfo"}}, "text": "Federated optimization has several key properties that differentiate it from a typical distributed optimization problem:\n\n\n\n \u2022 Non-IID:  The training data on a given client is typically based on the usage of the mobile device by a particular user, and hence any particular user\u2019s local dataset will not be representative of the population distribution.\n\n\n\n\u2022 Unbalanced Similarly: some users will make much heavier use of the service or app than others, leading to varying amounts of local training data.\n\n\n\n \u2022 Massively distributed: We expect the number of clients participating in an optimization to be much larger than the average number of examples per client.\n\n\n\n \u2022 Limited communication: Mobile devices are frequently offline or on slow or expensive connections. \n\n                                                                            Table 1: Literature Review\n\n\n\nS.no\n\nPaper Name\n\nDataset\n\nImplementation\n\nResults\n\n1.\n\n\tCommunication-Efficient Learning Deep Networks From Decentralized Data\n\nLarge-scale next-word prediction\n\n( 10 million public social media posts )\n\nFederatedAveraging Algorithm (FedAvg): Combines local SGD updates with global model averaging.\n\nAchieved 10.5% accuracy in 35 rounds with FedAvg (23\u00d7 fewer than FedSGD).\n\n2.\n\n\tFederated Learning of Large Language Models with Parameter-Efficient Prompt Tuning and Adaptive Optimization.\n\nQNLI, SST-2, CoLA, MRPC, RTE, and BoolQ.\n\nFedPepTAO: A parameter-efficient prompt tuning approach combined with adaptive optimization.\n\nOutperformed baselines by up to 60.8% in accuracy and reduced training time by up to 97.59%.\n\n3.\n\nFed-ensemble: Ensemble Models in Federated Learning for Improved Generalization and Uncertainty Quantification\n\nMNIST, FEMNIST, CIFAR-10, CIFAR-100, Shakespeare, OpenImage, 3D Printer Dataset\n\nTrains an ensemble of K models using random permutations\n\nOutperformed baseline fl methods like FedAvg, FedProx, and FedBe across datasets.\n\n4.\n\nARobust Privacy-Preserving Federated Learning Model Against Model Poisoning Attacks\n\nMNIST, KDDCup, Amazon Reviews\n\nThey introduce an internal auditor that evaluates encrypted gradient similarity and distribution to differentiate between benign and malicious gradients\n\nDemonstrates significant robustness against both targeted and untargeted attacks in IID and non-IID settings.\n\n5.\n\nOpenFedLLM: Training Large Language Models on Decentralized Private Data via Federated Learning\n\nAlpaca, Alpaca-GPT4, FinGPT, MedAlpaca, Code-Alpaca, MathInstruct, UltraFeedback, HH-RLHF\n\nIntroduced a research-friendly framework/codebase, named OpenFedLLM\n\nOutperformed state-of-the-art models like GPT-4 in domain specific \n\ndatasets\n\n\n\n\n\n6.\n\n\tFederated Learning for Breast Density Classification: A Real-World Implementation\n\n2D mammography and tomosynthesis images\n\nImplemented using the FederatedAveraging algorithm to aggregate model updates from decentralized client data.\n\n\n\n6.3% Improvement in accuracy on local test datasets.A45.8% improvement in generalizability on external test dataset\n\n\n\n\n\n\n\n\n\n7.\n\n\n\nTITANIC: Towards Production Federated Learning with Large Language Models\n\n\n\nWikitext-2-raw-v1 dataset\n\n( 36,700 text samples )\n\nA mechanism to partition LLMs and facilitate seamless forward and backward passes across client devices.\n\n\t\tachieved similar convergence trends to centralized training.\n\n\n\n\n\n8.\n\n\t\tFederated Large Language Models for Swarm Intelligence: A Survey\n\nGLUE, MRPC, SST2, and OntoNotes, IMDB, Yelp, and AGNEWS\n\nPpaper explores multiple federated learning techniques for LLMs in swarm intelligence contexts\n\nDoes not present new experimental results\n\n9.\n\n\tFederated Large Language Models: Current Progress and Future Directions \n\nGLUE, MRPC, SST2, Medical VQA, Weather Forecasting Data\n\nThe paper explores efficient federated fine-tuning methods privacy-preserving techniques, and communication-optimized frameworks\n\n Achieve performance comparable to centralized models when well-optimized.\n\n10.\n\nHeterogeneous Federated Learning: State-of-the-art and Research Challenges\n\n\t\t\tIoT data, healthcare data, and more\n\nThe paper categorizes HFL methods into data-level, model-level, and server-level strategies to address heterogeneity in data, models, and system coordination.\n\nDoes not present new experimental results\n\n11.\n\nModel Merging in LLMs, MLLMs, and Beyond: Methods, Theories, Applications, and Opportunities\n\nReviews existing works in the fields of large language models (LLMs)\n\nModel merging strategies span preparation techniques and advanced integration methods to optimize the fusion of diverse model capabilities.\n\n\n\nhighlights the practical applications and challenges of model merging across fields", "mimetype": "text/plain", "start_char_idx": 3231, "end_char_idx": 7901, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"8b216cc1-d4ca-4411-95cc-d2ea4a2ed248": {"node_ids": ["dc66679b-e240-4071-bd33-7e9b92bf53eb", "2c4e42d2-4860-47ae-ac62-41bfed88dbd8"], "metadata": {"file_name": "FL for LLM ResearchPaper.docx", "file_path": "C:\\Users\\Archit\\Desktop\\help\\llama_index-app\\app\\user_uploads\\FL for LLM ResearchPaper.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 15442, "creation_date": "2024-11-20", "last_modified_date": "2024-11-20"}}}}